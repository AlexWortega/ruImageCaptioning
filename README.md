# ruImageCaptioning



<a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/badge/License-MIT-yellow.svg"></a>  
Inference Notebook: <a href="https://colab.research.google.com/drive/1tsVMWUE6_AKXiHyinCSOSRPGhbjVEyRM?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=20></a>  

[Hugginfaceü§ó](https://huggingface.co/AlexWortega/ruImagecaptioning/tree/main)

–†—É—Å—Å–∫–∞—è –≤–µ—Ä—Å–∏—è CLIP prefix caption, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ ruGPTSMALL + CLIP(OPENAI), –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è VQA, image captioning –∏ –ø—Ä–æ—á–µ–µ. –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç <1c + –µ–µ –º–æ–∂–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∫–≤–∞–Ω—Ç–∞–Ω—É—Ç—å/–ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –≤ ONNX. –û–±—É—á–∞–ª–æ—Å—å –≤ —Ç–µ—á–µ–Ω–∏–∏ 3—Ö –¥–Ω–µ–π –Ω–∞ 2*1080ti

Trained and validated on ruCOCO
BLEU:     	37.3

chrF:     	32.4

ROUGE-1-F:	33.0

ROUGE-2-F:	14.1

ROUGE-L-F:	30.3

<table>
  <tr>
    <td><img src="images/photo_2021-11-13_13-08-38.jpg" ></td>
    <td><img src="images/–°–Ω–∏–º–æ–∫ —ç–∫—Ä–∞–Ω–∞ 2022-05-20 –≤ 15.54.33.png" ></td>
   
  </tr>
  <tr>
    <td> –ü—Ä–∏–º–µ—Ä –∫—ç–ø—à–µ–Ω–∏–Ω–≥–∞</td>
     <td>–ü—Ä–∏–º–µ—Ä —Ä–∞–±–æ—Ç—ã –≤ zero shot</td>
     
  </tr>
 </table>


THIS WORK IS BASED ON https://github.com/rmokady/CLIP_prefix_caption (english version)
```
@article{mokady2021clipcap,
  title={ClipCap: CLIP Prefix for Image Captioning},
  author={Mokady, Ron and Hertz, Amir and Bermano, Amit H},
  journal={arXiv preprint arXiv:2111.09734},
  year={2021}
}
```
```
@article{AlexWortega,
  title={ruImage captioning},
  author={Aleksandr Nikolic, Asta gpu server },
}
```

## Acknowledgments
This repository is heavily based on [CLIP](https://github.com/openai/CLIP) and [Hugging-faces](https://github.com/huggingface/transformers) repositories.
For training we used the data of [COCO dataset](https://cocodataset.org/#home)  and [Conceptual Captions](https://ai.google.com/research/ConceptualCaptions/) translated by ALEX WORTEGA [ruCOCO](https://github.com/AlexWortega/ru_COCO)  
